{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "majBlackFN = '../data/DURHAM/MajBlack.shp'\n",
    "keep1 = '../data/DURHAM/keep1.shp'\n",
    "clusters  = '../data/DURHAM/clusters.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in blocks and assign a unique index\n",
    "gdfBlocks = gpd.read_file('../data/DURHAM/DURHAM_blocks.shp')\n",
    "gdfBlocks[\"OrgID\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select blocks that are majority black\n",
    "gdfMajBlack = gdfBlocks.query('PctBlack >= 50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Of those, select blocks that have at least 50 BHH, these we'll keep (1)\n",
    "gdf_Org1 = gdfMajBlack.query('BlackHH > 50').reset_index()\n",
    "gdf_Org1.drop(['index', 'STATEFP10', 'COUNTYFP10', \n",
    "               'TRACTCE10', 'BLOCKCE', 'BLOCKID10',\n",
    "               'GEOID10','PARTFLG'],axis=1,inplace=True)\n",
    "gdf_Org1['OrgID'] = gdf_Org1.index + 1\n",
    "gdf_Org1['OrgType'] = 'OriginalBlock'\n",
    "gdf_Org1.to_file(keep1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Of those, select blocks that have fewer than 50 BHH; these we'll cluster\n",
    "gdfMajBlack_LT50 = gdfMajBlack.query('BlackHH < 50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster\n",
    "gdfClusters = gpd.GeoDataFrame(geometry = list(gdfMajBlack_LT50.unary_union))\n",
    "gdfClusters['ClusterID'] = gdfClusters.index\n",
    "gdfClusters.crs = gdfMajBlack_LT50.crs\n",
    "#gdfClusters.to_file('../data/DURHAM/clusters.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatially join the cluster ID to the original blocks\n",
    "gdfMajBlack_LT50_2 = gpd.sjoin(gdfMajBlack_LT50,gdfClusters,\n",
    "                               how='left',op='within').drop(\"index_right\",axis=1)\n",
    "#gdfMajBlack_LT50_2.to_file('../data/DURHAM/MajBlack1.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the total BHH for the dissolved blocks and add as block attribute\n",
    "gdfClusters_2 = gdfMajBlack_LT50_2.dissolve(by='ClusterID', aggfunc='sum')\n",
    "gdfClusters_2['PctBlack'] = gdfClusters_2['P003003'] / gdfClusters_2['P003001'] * 100\n",
    "gdfClusters_2['PctBlack18'] = gdfClusters_2['P010004'] / gdfClusters_2['P010001'] * 100\n",
    "\n",
    "#Remove block clusters with fewer than 50 BHH; these are impractical\n",
    "gdfClusters_2 = gdfClusters_2.query('BlackHH >= 50')\n",
    "#gdfClusters_2.to_file('../data/DURHAM/clusters2.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select clusters with fewer than 100 BHH, these we'll keep as org units(2)\n",
    "gdf_Org2 = gdfClusters_2.query('BlackHH <= 100').reset_index()\n",
    "gdf_Org2['OrgID'] = gdf_Org1['OrgID'].max() + gdf_Org2.index + 1\n",
    "gdf_Org2['OrgType'] = 'Full block cluster'\n",
    "gdf_Org2.to_file('../data/DURHAM/keep2.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of Cluster IDs for block clusters with more than 100 BHH;\n",
    "# we'll cluster individual blocks with these IDs until BHH >= 100\n",
    "clusterIDs = gdfClusters_2.query('BlackHH > 100').index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through each clusterID\n",
    "gdfs = []\n",
    "for clusterID in clusterIDs:\n",
    "    #Get the blocks in the cluster\n",
    "    gdfBlks = gdfMajBlack_LT50_2.query('ClusterID == {}'.format(clusterID)).reset_index()\n",
    "    #Get the first block in the block cluster\n",
    "    gdfBlks['X'] = gdfBlks.geometry.centroid.x\n",
    "    gdfNbrs = gdfBlks[gdfBlks['X'] == gdfBlks['X'].min()]\n",
    "    #Get the number of BHH and the geometry\n",
    "    BHH = gdfNbrs.BlackHH.sum()\n",
    "    geom = gdfNbrs.geometry.unary_union\n",
    "    \n",
    "    #Iterate while BHH <= 100\n",
    "    iter_count = 0\n",
    "    while BHH < 100 and iter_count < 100:\n",
    "        #Get the blocks that touch the current geometry\n",
    "        gdfNbrs = gdfBlks[gdfBlks.geometry.intersects(geom)]\n",
    "        #Compute the new aggregate BHH and Geometry\n",
    "        BHH = gdfNbrs.BlackHH.sum()\n",
    "        geom = gdfNbrs.geometry.unary_union\n",
    "        #Increae the iter_count to catch infinite loops\n",
    "        iter_count += 1\n",
    "        \n",
    "    #After 100 BHH are reached: select blocks intersecting the geom and assign an org unit ID\n",
    "    gdfSelect = (gdfBlks[gdfBlks.geometry.intersects(geom)]\n",
    "                 .reset_index()\n",
    "                 .dissolve(by='ClusterID', aggfunc='sum')\n",
    "                 .drop(['level_0','index','X'],axis=1)\n",
    "                )\n",
    "    \n",
    "    gdfs.append(gdfSelect)\n",
    "    \n",
    "\n",
    "#Combine each cluster into a single dataframe and write to a file\n",
    "gdf_Org3 = pd.concat(gdfs).reset_index()\n",
    "gdf_Org3['OrgID'] = gdf_Org2['OrgID'].max() + gdf_Org3.index + 1\n",
    "gdf_Org3['OrgType'] = 'Partial block cluster'\n",
    "gdf_Org3.to_file('../data/Durham/Keep3.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through each clusterID\n",
    "gdfs = []\n",
    "for clusterID in clusterIDs:\n",
    "    #Get the blocks in the cluster\n",
    "    gdfBlks = gdfMajBlack_LT50_2.query('ClusterID == {}'.format(clusterID)).reset_index()\n",
    "    #Set a switch to see if the block has been added yet\n",
    "    gdfBlks['claimed'] = 0\n",
    "    #Get the first block in the block cluster\n",
    "    gdfBlks['X'] = gdfBlks.geometry.centroid.x\n",
    "    gdfNbrs = gdfBlks[gdfBlks['X'] == gdfBlks['X'].min()]\n",
    "    #Get the number of BHH and the geometry\n",
    "    BHH = gdfNbrs.BlackHH.sum()\n",
    "    geom = gdfNbrs.geometry.unary_union\n",
    "    \n",
    "    #Iterate while BHH <= 100\n",
    "    iter_count = 0\n",
    "    while BHH < 100 and iter_count < 100:\n",
    "        #Get the blocks that touch the current geometry\n",
    "        gdfNbrs = gdfBlks[gdfBlks.geometry.intersects(geom) & gdfBlks.claimed == 0]\n",
    "        gdfBlks.loc[gdfBlks.geometry.intersects(geom),'claimed'] = 1\n",
    "        #Compute the new aggregate BHH and Geometry\n",
    "        BHH = gdfNbrs.BlackHH.sum()\n",
    "        geom = gdfNbrs.geometry.unary_union\n",
    "        #Increae the iter_count to catch infinite loops\n",
    "        iter_count += 1\n",
    "        \n",
    "    #After 100 BHH are reached: select blocks intersecting the geom and assign an org unit ID\n",
    "    gdfSelect = (gdfBlks[gdfBlks.geometry.intersects(geom)]\n",
    "                 .reset_index()\n",
    "                 .dissolve(by='ClusterID', aggfunc='sum')\n",
    "                 .drop(['level_0','index','X'],axis=1)\n",
    "                )\n",
    "    \n",
    "    gdfs.append(gdfSelect)\n",
    "    \n",
    "#Repeat, but with remaining unclaimed blocks with < 50 BHH\n",
    "gdfsClaimed = pd.concat(gdfs).reset_index()\n",
    "\n",
    "\n",
    "#Combine each cluster into a single dataframe and write to a file\n",
    "gdf_Org3 = pd.concat(gdfs).reset_index()\n",
    "gdf_Org3['OrgID'] = gdf_Org2['OrgID'].max() + gdf_Org3.index + 1\n",
    "gdf_Org3['OrgType'] = 'Partial block cluster'\n",
    "gdf_Org3.to_file('../data/Durham/Keep4.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all three keepers\n",
    "gdfAllOrgs = pd.concat((gdf_Org1, gdf_Org2, gdf_Org3),axis=0,sort=True)\n",
    "gdfAllOrgs.to_file('../data/DURHAM/Orgs.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
